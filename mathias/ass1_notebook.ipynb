{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "import regex as re\n",
    "re.findall(PAT, \"some text that i'll pre-tokenize\")\n",
    "\n",
    "# prefer to use re.finditer() to avoid storing the pre-tokenized words\n",
    "# as you construct your mapping from pre-tokens to their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"low low low low low \\\n",
    "lower lower widest widest widest \\\n",
    "newest newest newest newest newest newest\"\n",
    "\n",
    "pre_tokens = {\"low\": 5, \"lower\": 2, \"widest\": 3, \"newest\": 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('es', 9),\n",
      " ('st', 9),\n",
      " ('we', 8),\n",
      " ('t ', 8),\n",
      " ('lo', 7),\n",
      " ('ow', 7),\n",
      " (' l', 6),\n",
      " (' n', 6),\n",
      " ('ne', 6),\n",
      " ('ew', 6),\n",
      " ('w ', 5),\n",
      " (' w', 3),\n",
      " ('wi', 3),\n",
      " ('id', 3),\n",
      " ('de', 3),\n",
      " ('er', 2),\n",
      " ('r ', 2)]\n",
      "9\n",
      "['st', 'es']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pair_occurences = {}\n",
    "\n",
    "for i,_ in  enumerate(txt[:-1]):\n",
    "    pair_occurences[txt[i]+txt[i+1]] = pair_occurences.get((txt[i]+txt[i+1]), 0) + 1\n",
    "pair_occurences_sorted = sorted(pair_occurences.items(), key=lambda item: item[1], reverse=True)\n",
    "pprint(pair_occurences_sorted)\n",
    "\n",
    "max_count = pair_occurences_sorted[0][1]\n",
    "print(max_count)\n",
    "to_merge = []\n",
    "for pair in pair_occurences_sorted:\n",
    "    if pair[1] == max_count:\n",
    "        to_merge.append(pair[0])\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "to_merge = sorted(to_merge, reverse=True)\n",
    "print(to_merge)\n",
    "# pre_tokens = b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAT  \\p{L}|\\s\n",
      "['l', 'o', 'w', ' ', 'l', 'o', 'w', ' ', 'l', 'o', 'w', ' ', 'l', 'o', 'w', ' ', 'l', 'o', 'w', ' ', 'l', 'o', 'w', 'e', 'r', ' ', 'l', 'o', 'w', 'e', 'r', ' ', 'w', 'i', 'd', 'e', 's', 't', ' ', 'w', 'i', 'd', 'e', 's', 't', ' ', 'w', 'i', 'd', 'e', 's', 't', ' ', 'n', 'e', 'w', 'e', 's', 't', ' ', 'n', 'e', 'w', 'e', 's', 't', ' ', 'n', 'e', 'w', 'e', 's', 't', ' ', 'n', 'e', 'w', 'e', 's', 't', ' ', 'n', 'e', 'w', 'e', 's', 't', ' ', 'n', 'e', 'w', 'e', 's', 't']\n",
      "PAT  st|\\p{L}|\\s\n",
      "['l', 'o', 'w', ' ', 'l', 'o', 'w', ' ', 'l', 'o', 'w', ' ', 'l', 'o', 'w', ' ', 'l', 'o', 'w', ' ', 'l', 'o', 'w', 'e', 'r', ' ', 'l', 'o', 'w', 'e', 'r', ' ', 'w', 'i', 'd', 'e', 'st', ' ', 'w', 'i', 'd', 'e', 'st', ' ', 'w', 'i', 'd', 'e', 'st', ' ', 'n', 'e', 'w', 'e', 'st', ' ', 'n', 'e', 'w', 'e', 'st', ' ', 'n', 'e', 'w', 'e', 'st', ' ', 'n', 'e', 'w', 'e', 'st', ' ', 'n', 'e', 'w', 'e', 'st', ' ', 'n', 'e', 'w', 'e', 'st']\n",
      "PAT  est|st|\\p{L}|\\s\n",
      "['l', 'o', 'w', ' ', 'l', 'o', 'w', ' ', 'l', 'o', 'w', ' ', 'l', 'o', 'w', ' ', 'l', 'o', 'w', ' ', 'l', 'o', 'w', 'e', 'r', ' ', 'l', 'o', 'w', 'e', 'r', ' ', 'w', 'i', 'd', 'est', ' ', 'w', 'i', 'd', 'est', ' ', 'w', 'i', 'd', 'est', ' ', 'n', 'e', 'w', 'est', ' ', 'n', 'e', 'w', 'est', ' ', 'n', 'e', 'w', 'est', ' ', 'n', 'e', 'w', 'est', ' ', 'n', 'e', 'w', 'est', ' ', 'n', 'e', 'w', 'est']\n",
      "PAT  est |est|st|\\p{L}|\\s\n",
      "['l', 'o', 'w', ' ', 'l', 'o', 'w', ' ', 'l', 'o', 'w', ' ', 'l', 'o', 'w', ' ', 'l', 'o', 'w', ' ', 'l', 'o', 'w', 'e', 'r', ' ', 'l', 'o', 'w', 'e', 'r', ' ', 'w', 'i', 'd', 'est ', 'w', 'i', 'd', 'est ', 'w', 'i', 'd', 'est ', 'n', 'e', 'w', 'est ', 'n', 'e', 'w', 'est ', 'n', 'e', 'w', 'est ', 'n', 'e', 'w', 'est ', 'n', 'e', 'w', 'est ', 'n', 'e', 'w', 'est']\n",
      "PAT  ow|est |est|st|\\p{L}|\\s\n",
      "['l', 'ow', ' ', 'l', 'ow', ' ', 'l', 'ow', ' ', 'l', 'ow', ' ', 'l', 'ow', ' ', 'l', 'ow', 'e', 'r', ' ', 'l', 'ow', 'e', 'r', ' ', 'w', 'i', 'd', 'est ', 'w', 'i', 'd', 'est ', 'w', 'i', 'd', 'est ', 'n', 'e', 'w', 'est ', 'n', 'e', 'w', 'est ', 'n', 'e', 'w', 'est ', 'n', 'e', 'w', 'est ', 'n', 'e', 'w', 'est ', 'n', 'e', 'w', 'est']\n",
      "PAT  low|ow|est |est|st|\\p{L}|\\s\n",
      "['low', ' ', 'low', ' ', 'low', ' ', 'low', ' ', 'low', ' ', 'low', 'e', 'r', ' ', 'low', 'e', 'r', ' ', 'w', 'i', 'd', 'est ', 'w', 'i', 'd', 'est ', 'w', 'i', 'd', 'est ', 'n', 'e', 'w', 'est ', 'n', 'e', 'w', 'est ', 'n', 'e', 'w', 'est ', 'n', 'e', 'w', 'est ', 'n', 'e', 'w', 'est ', 'n', 'e', 'w', 'est']\n",
      "step merge ['st', 'est', 'est ', 'ow', 'low', 'ne']\n",
      "[(' low', 6),\n",
      " ('est n', 6),\n",
      " ('ne', 6),\n",
      " ('ew', 6),\n",
      " ('low ', 5),\n",
      " ('west ', 5),\n",
      " ('wi', 3),\n",
      " ('id', 3),\n",
      " ('dest ', 3),\n",
      " ('lowe', 2),\n",
      " ('er', 2),\n",
      " ('r ', 2),\n",
      " ('est w', 2),\n",
      " (' w', 1),\n",
      " ('west', 1)]\n"
     ]
    }
   ],
   "source": [
    "PAT =  r\"\"\"\\p{L}|\\s\"\"\"\n",
    "step_merge = []\n",
    "txt = \"low low low low low \\\n",
    "lower lower widest widest widest \\\n",
    "newest newest newest newest newest newest\"\n",
    "# for match in re.finditer(PAT,txt):\n",
    "    # print(match.group())\n",
    "    \n",
    "for iteration in range(6):\n",
    "    print(\"PAT \", PAT)\n",
    "    pair_occurences = {}\n",
    "    new_txt = re.findall(PAT, txt)\n",
    "    print(new_txt)\n",
    "    for i,_ in enumerate(new_txt[:-1]):\n",
    "        pair_occurences[new_txt[i]+new_txt[i+1]] = pair_occurences.get((new_txt[i]+new_txt[i+1]), 0) + 1\n",
    "    pair_occurences_sorted = sorted(pair_occurences.items(), key=lambda item: item[1], reverse=True)\n",
    "    # pprint(pair_occurences_sorted)\n",
    "\n",
    "    max_count = pair_occurences_sorted[0][1]\n",
    "    # print(max_count)\n",
    "    to_merge = []\n",
    "    for pair in pair_occurences_sorted:\n",
    "        if pair[1] == max_count:\n",
    "            to_merge.append(pair[0])\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    to_merge = sorted(to_merge, reverse=True)\n",
    "    # print(\"to_merge \", to_merge)\n",
    "    step_merge.append(to_merge[0])\n",
    "    PAT =  rf'{to_merge[0]}' + \"|\" + PAT\n",
    "\n",
    "print(\"step merge\", step_merge)\n",
    "pprint(pair_occurences_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m txt \u001b[38;5;241m=\u001b[39m txt\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m      2\u001b[0m PAT \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{L}\u001b[39;00m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m new_txt \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(PAT, txt)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/regex/regex.py:338\u001b[0m, in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags, pos, endpos, overlapped, concurrent, timeout, ignore_unused, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of all matches in the string. The matches may be overlapped\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mif overlapped is True. If one or more groups are present in the pattern,\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03mreturn a list of groups; this will be a list of tuples if the pattern has\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03mmore than one group. Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m pat \u001b[38;5;241m=\u001b[39m _compile(pattern, flags, ignore_unused, kwargs, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pat\u001b[38;5;241m.\u001b[39mfindall(string, pos, endpos, overlapped, concurrent, timeout)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "txt = txt.split()\n",
    "PAT =  r\"\"\"\\p{L}|\\s\"\"\"\n",
    "new_txt = re.findall(PAT, txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_train_tokenizer(input_path:str, vocab_size:int, special_tokens: list[str])-> tuple[dict[int, bytes],list[tuple[bytes, bytes]]] :\n",
    "    \"\"\"\n",
    "    returns:\n",
    "        - vocab Dict[int, bytes]: The tokenizer vocabulary, a mapping from a int (token ID in the vocabulary) to bytes (token bytes)\n",
    "        - merges List[Tuple[bytes, bytes]]: A list of BPE merges produced from training. Each list item is a tuple of bytes (<token1>, <token2>),\n",
    "            where <token1> and <token2> are the tokens that were merged. The merges should be ordered by order of creation\n",
    "    \n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
